{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Initialization\n",
    "## Load libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation\n",
    "from captum.attr import LayerConductance\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pprint\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Custom\n",
    "from tool_box.model import DenseNN\n",
    "from tool_box.utilities import pair_plot_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set timestamp for run ID\n",
    "run_timestamp = datetime.datetime.now()\n",
    "\n",
    "torch.manual_seed(44)\n",
    "pp = pprint.PrettyPrinter(width=41, compact=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Config\n"
    }
   }
  },
  {
   "source": [
    "# Load and prepare data\n",
    "## Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./source/INCART 2-lead Arrhythmia Database.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Redefine target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['anomaly_ind'] = 0\n",
    "df.loc[df.type != 'N', 'anomaly_ind'] = 1\n",
    "df = df.drop(['type'], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove non-used columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(['record'], axis=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "any(df.isnull().sum() != 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby('anomaly_ind').agg({'0_pre-RR':'count'}).rename(columns={'0_pre-RR':'Count'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp_df = pair_plot_sample(df, 'anomaly_ind', 1000)\n",
    "\n",
    "pair_comb = (\n",
    "    ('RR'),\n",
    "    ('Peak'),\n",
    "    ('interval'),\n",
    "    ('morph0', 'morph1', 'morph2', 'morph3', 'morph4'),\n",
    ")\n",
    "\n",
    "for c in pair_comb:\n",
    "\n",
    "    fig = px.scatter_matrix(\n",
    "        pp_df,\n",
    "        dimensions=[col for col in pp_df.columns if col.endswith(c)],\n",
    "        title='Relation between metrics',\n",
    "        color_continuous_scale='Bluered_r',\n",
    "        color='anomaly_ind',\n",
    "        opacity=0.05,\n",
    "        width=1500, height=900\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "# Prepare data\n",
    "## Create features and target datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = df.anomaly_ind.to_numpy()\n",
    "features_names = list(df.columns)\n",
    "features = df.drop(['anomaly_ind'], axis=1).to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "## Scale data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "features = scaler.fit_transform(features)"
   ]
  },
  {
   "source": [
    "## Splitting data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TEST_FRAC = 0.3\n",
    "samples = len(target)\n",
    "\n",
    "train_indices = np.random.choice(samples, int((1 - TEST_FRAC)*samples), replace=False)\n",
    "test_indices = list(set(range(samples)) - set(train_indices))\n",
    "\n",
    "train_label, train_feature = target[train_indices], features[train_indices]\n",
    "test_label, test_feature = target[test_indices], features[test_indices]\n",
    "\n",
    "print(f'Training samples: {train_feature.shape[0]}\\nTesting samples: {test_feature.shape[0]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transform data into tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_feature_tensor = torch.from_numpy(train_feature).type(torch.FloatTensor)\n",
    "train_label_tensor = torch.from_numpy(train_label).type(torch.FloatTensor)\n",
    "\n",
    "test_feature_tensor = torch.from_numpy(test_feature).type(torch.FloatTensor)\n",
    "test_label_tensor = torch.from_numpy(test_label).type(torch.FloatTensor)\n",
    "\n",
    "train_data = list(zip(train_feature_tensor, train_label_tensor))\n",
    "test_data = list(zip(test_feature_tensor, test_label_tensor))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "## Create Dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2**9\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "source": [
    "# Model\n",
    "## Create model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dense_nn = DenseNN(\n",
    "    train_feature_tensor.shape[1],\n",
    "    train_feature_tensor.shape[1]*2+1,\n",
    "    train_feature_tensor.shape[1]*1+1,\n",
    "    1,\n",
    "    criterion = nn.BCELoss(),\n",
    "    optimizer = 'adam',\n",
    "    learning_rate=0.01\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "### Run model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loss_dict = dict()\n",
    "test_loss_dict = dict()\n",
    "num_epochs = 300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss_lst = list()\n",
    "    test_loss_lst = list()\n",
    "\n",
    "    for i, train_batch in enumerate(train_loader):\n",
    "\n",
    "        train_loss = dense_nn.training_step(train_batch)\n",
    "        train_loss_lst.append(train_loss)\n",
    "\n",
    "    for i, test_batch in enumerate(test_loader):\n",
    "\n",
    "        test_loss = dense_nn.testing_step(test_batch)\n",
    "        test_loss_lst.append(test_loss)\n",
    "\n",
    "    train_loss_dict[epoch] = train_loss_lst\n",
    "    test_loss_dict[epoch] = test_loss_lst\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print ('Epoch {}/{} => Train loss: {:.3f} - Test loss: {:.3f}'.format(\n",
    "            epoch+1,\n",
    "            num_epochs,\n",
    "            np.mean(train_loss_dict[epoch]),\n",
    "            np.mean(test_loss_dict[epoch])\n",
    "            )\n",
    "        )\n",
    "\n",
    "model_path = f\"./models/dense_nn_{run_timestamp.strftime('%Y%m%d%H%M%S')}.pt\"\n",
    "torch.save(dense_nn.state_dict(), model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = dense_nn.model_eval(train_feature_tensor, train_label_tensor, test_feature_tensor, test_label_tensor)\n",
    "pp.pprint(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_train_loss_lst = [item for sublist in train_loss_dict.values() for item in sublist[:len(test_loss_dict[0])]]\n",
    "sample_test_loss_lst = [item for sublist in test_loss_dict.values() for item in sublist]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(sample_train_loss_lst))), y=sample_train_loss_lst, name= 'Training', mode='lines', line_color='blue', opacity=1)\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(sample_test_loss_lst))), y=sample_test_loss_lst, name= 'Testing', mode='lines', line_color='red', opacity=0.5)\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Average loss by epoch\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"Loss\",\n",
    "    font=dict(size=15),\n",
    "    width=1300,\n",
    "    height=600)\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "# Explainable IA\n",
    "## Set up attribution algorithms"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_test_indices = torch.randperm(test_feature_tensor.size(0))[:3000]\n",
    "sample_test_feature_tensor = test_feature_tensor[sample_test_indices]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attr_algos = {\n",
    "    'IntegratedGradients': dict(),\n",
    "    'DeepLift': dict(),\n",
    "    'FeatureAblation': dict()\n",
    "}\n",
    "\n",
    "for algo in attr_algos:\n",
    "    attr_algos[algo]['algorithm'] = eval(f\"{algo}(dense_nn)\")\n",
    "    attr_algos[algo]['attr'] = attr_algos[algo]['algorithm'].attribute(sample_test_feature_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "## Feature importance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_labels = list(df.drop('anomaly_ind', axis=1).columns)\n",
    "\n",
    "for algo in attr_algos:\n",
    "    attr_algos[algo]['sum'] = attr_algos[algo]['attr'].detach().numpy().sum(axis=0)\n",
    "    attr_algos[algo]['euc_norm'] = np.linalg.norm(attr_algos[algo]['sum'], ord=1)\n",
    "    attr_algos[algo]['norm_vec'] = attr_algos[algo]['sum'] / attr_algos[algo]['euc_norm']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for algo in attr_algos:\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=x_labels, y=attr_algos.get(algo).get('norm_vec'), name=algo)\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attributions, approximation_error = attr_algos.\\\n",
    "    get('IntegratedGradients').\\\n",
    "    get('algorithm').\\\n",
    "    attribute(test_feature_tensor[0].reshape(1, -1), return_convergence_delta = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=x_labels, y=attributions.numpy().reshape(-1,), name='IntegratedGradients')\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "approximation_error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "layer_2 = LayerConductance(dense_nn, dense_nn.z2)\n",
    "layer_2_attr = layer_2.attribute(sample_test_feature_tensor, n_steps=50, attribute_to_layer_input=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "layer_2_gral_attr = layer_2_attr.mean(axis=0).detach().numpy()\n",
    "layer_2_norm_attr = layer_2_gral_attr / np.linalg.norm(layer_2_gral_attr, ord=1)\n",
    "\n",
    "layer_2_gral_weights = dense_nn.z2.weight.mean(axis=0).detach().numpy()\n",
    "layer_2_norm_weights = layer_2_gral_weights / np.linalg.norm(layer_2_gral_weights, ord=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neurons = ['Neuron ' + str(l+1) for l in range(len(layer_2_norm_weights))]\n",
    "neurons_values = {\n",
    "    'Weights':layer_2_norm_weights,\n",
    "    'Attributions':layer_2_norm_attr\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for val in neurons_values:\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=neurons, y=neurons_values.get(val), name=val)\n",
    "    )\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
